# Initial Meeting Experimental Plan

## Research Hypotheses & Practical Constraints

| Label  | Statement                                                    |
| ------ | ------------------------------------------------------------ |
| **H1** | *Well‑configured CNN trainings on CIFAR‑10 display a single, straight power‑law loss segment after burn‑in.* |
| **H2** | *The early‑segment slope α (epochs ≈ 5‑15) predicts the final best validation cross‑entropy (CE) within 50 epochs.* |

**Constraints before weekly advisor meeting**

- Run a sweep that finishes in ≤ 12 hours per GPU across 24 GPUs.
- Every design choice must be justified **empirically in our own setting**, not borrowed untested.
- Produce **two primary slides** (H1, H2) + evidence backup slides.
- Experiments must integrate seamlessly with the future Tracks (A: control, B: generalisation, C: noise).



## Design‑Decisions

Each row lists the **chosen setting**, why we chose it, a literature pointer, and the *mini‑experiment* (metric + slide) that justifies the choice at meeting time.

| ID      | Decision                                                     | Rationale                                                    | Related work                                                 | Validation snippet (metric → slide)                          |
| ------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **D1**  | **Architecture**: ResNet‑18; ablate BN ∈ {on, off}, width ∈ {1×, 0.5×}. | BN & width might alter smoothness/linearity; cheap two‑factor ablation. | BN stabilises gradients (Ioffe & Szegedy 15); width affects effective depth. | % runs with R² ≥ 0.98 vs setting → **Backup H**              |
| **D2**  | **Dataset & Augments**: CIFAR‑10, RandomResizedCrop + horizontal‑flip. | Minimal noise; matches most prior slope studies.             | He 15, Gowers 23.                                            | Show that α variance across seeds < RandAug variant → methods footnote |
| **D3**  | **Optimiser**: SGD+momentum (main) and AdamW (control).      | Checks optimiser‑invariance of α.                            | Goyal 17; Loshchilov 19.                                     | Scatter α vs CE for both optims → **Backup H**               |
| **D4**  | **LR schedule**: Cosine‑anneal, 5‑epoch warm‑up.             | Cosine is SOTA; warm‑up avoids early burst.*¹                | Smith 17; He 19.                                             | Knee distribution before/after warm‑up → **Backup A**        |
| **D5**  | **Burn‑in exclusion rule**: Analyse after CE ≤ 1.0 *or* detector knee. | Replicates earlier power‑law papers yet keeps rule data‑driven. | Zhang 21.                                                    | Box‑plot of knee epochs → **Backup A**                       |
| **D6**  | **Fit families tested**: exponential, single‑power, two‑power. | Demonstrate power‑law superiority, rule out over‑fitting.    | Luo 25 (two‑power); Hoffer 17 (exp tail).                    | ΔAIC bar plot → **Backup B**                                 |
| **D7**  | **Estimator**: OLS, Huber fallback (δ = 1.5σ).               | Huber guards against heavy‑tailed residuals.                 | Brandfonbrener 24.                                           | QQ‑plot + Δα table → **Backup E**                            |
| **D8**  | **Early‑slope window**: epochs 5‑15; ±5 robustness sweep.    | Matches warm‑up length; still early.                         | Simşekli 20.                                                 | ρ(α,CE) vs window heat‑map → **Backup C**                    |
| **D9**  | **Straightness threshold**: R² ≥ 0.98.                       | Ensures near‑linear fits; same cut‑off as Brandfonbrener.    | ibid.                                                        | R² histogram → **Primary 1**                                 |
| **D10** | **Performance target**: Best **validation** CE in first 50 epochs. | Advisor prefers val‑set metrics; fixed horizon.              | standard.                                                    | Scatter α vs best‑val‑CE → **Primary 2**                     |
| **D11** | **Hyper‑grid**: LR{0.05,0.1,0.2} × WD{1e‑4,1e‑3,1e‑2} × 6 seeds × arch 4 = 216 runs. | Adequate coverage; < 12 GPU‑h.                               | –                                                            | Heat‑map CE vs LR×WD → **Backup G**                          |
| **D12** | **Logging**: every ¼ epoch; store CE(nats & bits), acc, α_early, α_full, R², knee_epoch. | Supports all backup plots *and* Tracks A–C.                  | –                                                            | Table of log keys in appendix                                |

*¹ *Footnote:* warm‑up length is itself a lit‑assumption you may validate if α drifts during first 10 epochs.



## Slide Inventory

| ID            | Slide title                             | Plot(s) & metric(s)                                          | Decision IDs touched |
| ------------- | --------------------------------------- | ------------------------------------------------------------ | -------------------- |
| **Primary 1** | *Power‑law prevalence supports H1*      | (a) 6‑run curve mosaic; (b) R² histogram.                    | D1‑D6, D9            |
| **Primary 2** | *Early slope predicts performance (H2)* | Scatter $\alpha_{early}$ vs best‑val‑CE; overlay $\alpha_{full}$. | D8, D10              |
| Backup A      | Burn‑in & knee                          | CE curve + knee marker; knee epoch box‑plot.                 | D4‑D5                |
| Backup B      | Fit family AIC                          | ΔAIC exp vs power vs two‑power bars.                         | D6                   |
| Backup C      | Window robustness                       | Heat‑map ρ window start/stop.                                | D8                   |
| Backup E      | OLS vs Huber                            | QQ‑plot residuals; Δα bar.                                   | D7                   |
| Backup G      | Hyper‑grid sanity                       | CE heat‑map (LR×WD).                                         | D11                  |
| Backup H      | Arch & optimiser ablation               | % linear runs and α↔CE ρ for four arch combos + AdamW.       | D1, D3               |



## Meeting Narrative

1. **Open with H1 in a single sentence**. Show Primary 1 and say:
    “Across 216 diverse runs, > 90 % exhibit a clean power‑law segment—with R² distribution tightly peaked near 1—validating the linearity assumption.”
2. **Transition to H2**. Display Primary 2:
    “Measuring slope just 10 epochs in predicts the end‑of‑training validation CE with ρ ≈ 0.78, **equivalent or superior** to full‑segment slope.”

**Delivery tips**

- Keep each backup slide to **≤ 45 seconds** explanation.
- Practise the **bridge statements** (“As you can see on Backup B…”) so you can smoothly jump when challenged.

**Additional Steps for Meeting Prep**

- TODO: think about how to intro the meeting as it relates to last meeting (something like, I’d request you disregard what I said last meeting and starting from a blank slate I’ll provide a much better presentation of my experimental conclusions regarding the hypotheses we discussed.)
- TODO: identify the related work I’d point to in order to position this work, including the specific statement that I’d use to describe each.  Include in slides and be ready to expand on my description and defend.
- TODO: think through any math that he might pivot to in order to discuss different aspects of this, ensure I’m ready for that
  - TODO: go through the math he showed last week and understand it + ensure that you incorporate it
- TODO: think through different ways to frame results (eg. descriptive empirical, descriptive theoretical,  implications empirical, implications theoretical, other)



## Metrics to Log for Future Tracks (cheap extra columns)

| Track | Extra signals recommended now                                | Reason                                            |
| ----- | ------------------------------------------------------------ | ------------------------------------------------- |
| **A** | online λ₁, Tr H via Hutch++ every 500 iters                  | needed for Curvature Plateau & Fusion controllers |
| **B** | CIFAR‑10‑C / 10.1 *val* CE at epoch 50                       | enables Zero‑Cost Robustness Auditor              |
| **C** | PSD tail‑index, gradient variance, residual‑band power every epoch 2 | fuels Noise‑Metric Show‑Down                      |

All add < 15 % runtime overhead (fits 12 GPU‑h budget).



## Execution Checklist (chronological)

| Day             | Action                                                       | Output                        |
| --------------- | ------------------------------------------------------------ | ----------------------------- |
| **0**           | Finalise YAML‑template & Hydra config; check BN/width flags. | `configs/sweep_base.yaml`     |
| 0               | Implement `KneeDetectorCallback`, `SlopeLoggerCallback`, Huber fallback. | `callbacks/loss_fit.py`       |
| 0               | Test one run locally; verify log columns & wandb chart.      | sanity notebook               |
| **1**           | Launch 216‑job SLURM array (`sbatch sweep.sh 0‑215`).        | jobs running                  |
| 1‑2             | Monitor GPU utilisation; patch defects.                      | –                             |
| **2**           | Run quick notebook to compute α_early, update R² histogram.  | preliminary Primary 1         |
| **3**           | Compute α_full, produce α vs CE scatter.                     | preliminary Primary 2         |
| **4**           | Generate Backup A‑C, E, G plots; run knee/α invariance checks. | pdf of backup slides          |
| **4**           | Fill slide deck skeleton (Keynote / LaTeX Beamer).           | `slides/2025‑07‑advisor.pptx` |
| **5**           | Rehearse narrative + timing; add footnote slide as needed.   | –                             |
| **Meeting day** | Present; note advisor feedback & required follow‑ups.        | meeting notes                 |

------

### Footnotes & Caveats

1. *Warm‑up length*: if advisor objects to fixed 5 epochs, rerun window analysis with warm‑up excluded by detector instead.
2. *λ₁ logging*: validate eigenvalue estimates on **one** GPU first to ensure < 10 % wall‑time hit.
3. *Huber δ*: standard choice δ = 1.5·median |residual|; adjust only if > 5 % fits fall back.
4. *BN‑off runs*: may diverge for LR ≥ 0.2; add gradient‑clip 1.0 if needed.
5. *Metrics storage*: convert CSV → Parquet nightly to avoid > 250 GB local logs.

# Implementation Plan

Below is a **run sheet + analysis playbook** you can hand to a lab mate, launch un‑attended on SLURM, and later mine for every slide in the H1/H2 deck **and** for future Tracks A‑C.

Everything is organised around *job sets* (groups of runs with identical hyper‑grids) and *log schemas* (the raw metrics to store).  All heavy computations (slopes, AIC, correlations, λ₁, PSD tail) are deferred to **offline notebooks**, so the only runtime cost is writing basic scalars every ¼ epoch.

------

## 1 ‑ Exact experiments to run

| Job set      | Purpose            | Hyper‑grid                                                   | # runs |
| ------------ | ------------------ | ------------------------------------------------------------ | ------ |
| **S‑core**   | Baseline for H1/H2 | **Arch** = ResNet‑18/BN/on/width 1×; **optim** = SGD.LR ∈ {0.05, 0.10, 0.20}; WD ∈ {1e‑4, 1e‑3, 1e‑2}; seeds ∈ {0…5}. | 54     |
| **S‑adamw**  | Optimiser ablation | (same grid) AdamW, β₁ = 0.9, β₂ = 0.999.                     | 54     |
| **S‑bn‑off** | BN effect          | BN **off**, SGD grid.                                        | 54     |
| **S‑narrow** | Width effect       | width 0.5×, BN **on**, SGD grid.                             | 54     |

**TOTAL:** 216 runs.
 Each run: cosine LR, 5‑ep warm‑up, 50 epochs → ≈ 0.5 GPU‑h.
 105 GPU‑h ≈ **2 h 15 min wall clock** on 24 GPUs (2 jobs/GPU).

------

### 1.1 Minimal SLURM template (two workers per A100‑48GB)

```bash
#!/bin/bash
#SBATCH -p gpu
#SBATCH --gres=gpu:2
#SBATCH -N1
#SBATCH --array=0-215
CONFIGS=( $(python configs/make_grid.py) )  # returns 216 yaml paths
srun python train.py ${CONFIGS[$SLURM_ARRAY_TASK_ID]}
```

------

## 2 ‑ Runtime logging schema (raw only)

| Key                                   | Frequency           | Comment                            |
| ------------------------------------- | ------------------- | ---------------------------------- |
| `step`, `epoch`, `time`               | every **batch**     | anchor for resampling              |
| `lr`, `wd`                            | batch               | sanity audit                       |
| `loss_train_nats`, `loss_train_bits`  | batch               | convert bits = nats / ln 2         |
| `acc_train`                           | batch               | coarse health                      |
| `loss_val_nats`, `acc_val`            | **¼ epoch**         | 4× per epoch keeps logs small      |
| `grad_norm`, `weight_norm`            | ¼ epoch             | helps debug BN‑off runs            |
| `lambda_max`, `hutch_trace`           | every **500 iters** | Hutch++ k = 50, power‑iter 5 iters |
| `psd_tail`, `grad_var`, `resid_power` | every **epoch 2**   | PSD via 512‑sample FFT             |
| `checkpoint_size_mb`                  | epoch               | feeds disk‑usage stats             |

- **Keep** per-minibatch logging for **loss_train_nats/bits, acc_train, lr, wd** and timestamps.
- Retain **¼-epoch** cadence for *validation* metrics and for heavier signals (grad_norm, λ₁, PSD) to avoid unnecessary compute.
- Down-sample on-the-fly in notebooks (e.g., `df[::8]`) when plotting.

------

## 3 ‑ Post‑run offline analyses & slide hooks

| Analysis (script)         | Uses raw columns           | Output figure/metric                    | Slide & interpretation hint   | Gotchas                                              |
| ------------------------- | -------------------------- | --------------------------------------- | ----------------------------- | ---------------------------------------------------- |
| **A‑knee.py**             | loss_train_nats, epoch     | knee_epoch via ΔAIC exp vs power        | Backup A box‑plot             | knee mis‑detected if diverging runs; filter `loss>5` |
| **A‑fits.py**             | loss_train_nats after knee | exp / power / 2‑power params, AIC       | Backup B bars                 | ensure ≥ 15 points per fit segment                   |
| **A‑slope.py**            | same                       | α_early (ep 5‑15), α_full (knee‑50), R² | Primary 1 & 2, Backup C       | remove epochs < knee for α_full                      |
| **A‑window_scan.py**      | α window sweep             | heat‑map ρ(α,CE)                        | Backup C                      | watch Spearman vs Pearson difference                 |
| **A‑unit_inv.py**         | α(nats), α(bits)           | scatter                                 | footnote                      | slope≃1; flag if                                     |
| **A‑optim_arch.py**       | all                        | table: % linear, ρ per arch/optim       | Backup H                      | BN‑off may crash; mask NaNs                          |
| **A‑grid_heat.py**        | CE_val                     | heat‑map LR×WD                          | Backup G                      | smooth by median over seeds                          |
| **A‑noise_corr.py**       | psd_tail et al.            | corr matrix vs CE                       | holds for Track C             | λ > 0.8 -> potential proxy                           |
| **A‑curvature_timing.py** | lambda_max, knee           | scatter λ₁‑plateau vs α onset           | seeds Curvature Plateau study | λ₁ noisy early; use EWMA                             |

All scripts write `.png` into `plots/<script_name>/runset.png`, leaving LaTeX paths stable.

------

## 4 ‑ Interpreting key outputs

- **R² Histogram** (Primary 1).
   *Success* = > 80 % of runs with R² ≥ 0.98 → supports H1.
   *Failure* → examine BN‑off subset first; instability often due to exploding loss.
- **α early vs CE Scatter** (Primary 2).
   *Success* = |ρ| ≥ 0.7 with p < 0.01.
   If ρ < 0.4 → pivot to λ₁‑plateau predictor (needs lambda_max already logged).
- **ΔAIC Bar Plot**.
   ∆AIC < −2 for power vs exp = decisive evidence; if exp wins before epoch 10, shorten α window.
- **Window Heat‑map**.
   Flat plateau of ρ across ±5 epochs = robustness; steep drop signals warm‑up bleed‑through.
- **Arch Ablation Bars**.
   If BN‑off drops linear‑run % by > 30 pp, highlight as “bad setup” proof for advisor.

------

## 5 ‑ Steps to finished product (chronological)

1. **Generate 216 Hydra configs** (`make_grid.py`) and push to Git.
2. **Smoke‑test logging** on 1 config; confirm all keys exist.
3. **Submit SLURM array**; monitor dashboard for stragglers or NaNs.
4. While jobs run, **draft slide shells** (titles + figure placeholders).
5. As soon as run group completes, run **A‑knee.py → A‑fits.py → A‑slope.py** to populate Primary slides.
6. Fill Primary 1 & 2 with first‑pass plots; rehearse 3‑min narrative.
7. Run remaining scripts in background; drop plots into backup slides.
8. **Iterate**: if α‑CE ρ is weak, tweak window in `A‑window_scan.py`, regenerate scatter/hist.
9. Finalise deck; double‑check citation numbers and axis labels.
10. Archive raw logs to Parquet; keep 3 sample checkpoints for future Tracks.

------

### Quick‑check metric list (copy into logging code)

```python
LOG_KEYS = [
    "step","epoch","time",
    "lr","wd",
    "loss_train_nats","loss_train_bits","acc_train",
    "loss_val_nats","acc_val",
    "grad_norm","weight_norm",
    "lambda_max","hutch_trace",
    "psd_tail","grad_var","resid_power",
    "checkpoint_size_mb",
]
```

Keep it verbatim so post‑run notebooks parse seamlessly.

# Metric‑Instrumentation Reference

This note tells you, for **every metric you plan to log**, how to wire it into the existing Lightning‑based trainer, which Python tools to use for the calculation, and a quick validation checklist you must tick **before** launching the 216‑run sweep.  Follow the order below when you implement new callbacks.

Add in obvious places:

- Step
- Epoch
- Time 

## Optimiser hyper‑params

**lr**

- **Hook** – In Lightning’s `on_train_batch_start`, grab the first parameter group:

  ```python
  lr = self.trainer.optimizers[0].param_groups[0]["lr"]
  self.log_dict({"lr": lr}, on_step=True)
  ```

- **Validation** – Plot `lr` vs `step` for a single run; confirm cosine curve and warm‑up ramp.  Plot wd similarly, should be constant??

## Basic training signals

**loss_train_nats, loss_train_bits, acc_train**

- **Hook** – Already produced by the Lightning module’s `training_step`.  Add:

  ```python
  self.log("loss_train_bits", loss / math.log(2), on_step=True)
  ```

- **Validation** – During a 10‑epoch smoke‑test ensure the nats→bits conversion produces identical curves shifted by `ln 2`.

## Validation signals

**loss_val_nats, acc_val**

- **Hook** – Default Lightning `validation_step`, but set `log_every_n_steps` to a quarter of the CIFAR iterator length (≈ 100).
- **Validation** – Check there are exactly **4 rows per epoch** for these keys.

## Norm diagnostics

**grad_norm, weight_norm**

- **Hook** – In `on_after_backward`:

  ```python
  total_norm = torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=float("inf")).item()
  weight_norm = torch.norm(torch.stack([p.norm() for p in self.parameters()])).item()
  self.log_dict({"grad_norm": total_norm, "weight_norm": weight_norm}, on_step=False, on_epoch=True)
  ```

- **Validation** – Plot grad_norm vs epoch; it should decay once LR cosine decays.  Spikes may reveal exploding grads when BN is off.

## Curvature estimates

**lambda_max, hutch_trace**

- **Hook** – Add a `CurvatureCallback` that fires *every 500 optimiser steps* (adjustable via Hydra).  Use **`hessian_eigenthings`** (MIT‑licensed) or **`pyhessian`** for largest‑eigenvalue power iteration; use Hutch++ for trace:

  ```python
  from hessian_eigenthings import compute_hessian_eigenthings
  from hutch_nn import hutchinson_trace
  
  lam, _ = compute_hessian_eigenthings(model, dataloader, num_eigenthings=1, use_cuda=True)
  trace = hutchinson_trace(model, dataloader, num_samples=50)
  self.log_dict({"lambda_max": lam[0], "hutch_trace": trace}, on_step=False, on_epoch=True)
  ```

- **Validation** –

  1. Run on a single GPU and ensure wall‑time overhead per 500 steps is < 10 %.
  2. Plot λ₁ vs epoch for one run; expect initial rise then plateau/fall.

*Footnote ① λ₁ is noisy for small batch sizes; apply `torch.tensor(lam).ewm(alpha=0.3).mean()` before logging if variance is too high.*

## Noise & residual metrics

**psd_tail, grad_var, resid_power**

- **Hook** – Create a `NoiseMetricsCallback` that (i) buffers the last 512 **batch loss residuals** `ε_t = loss_t − ŷ_t` where `ŷ_t` is your online power‑law prediction, (ii) at *every 2 epochs* computes:

  ```python
  freqs = torch.fft.rfft(torch.tensor(buffer))
  psd = freqs.abs() ** 2
  psd_tail = psd[-16:].mean().item()          # high‑frequency band
  grad_var = running_grad_sq.mean() - running_grad.mean()**2
  resid_power = psd[8:32].sum().item()        # mid‑band
  self.log_dict({"psd_tail": psd_tail,
                 "grad_var": grad_var,
                 "resid_power": resid_power}, on_step=False, on_epoch=True)
  ```

  Use `torch.autograd.grad(loss, params, retain_graph=True)` in a **no‑grad** block to accumulate `running_grad`.

- **Validation** –
   • Over 10 epochs, verify `psd_tail` drops when LR decays (less high‑freq noise).
   • Compare `grad_var` between SGD and AdamW—AdamW should be lower early on.

*Footnote ② FFT needs constant sampling; if you skip batches (e.g., gradient accumulation) pad with previous value.*

## 8. Checkpoint size

**Middle‑ground checkpointing strategy (recommended).**

1. Set `save_top_k=1` in Lightning’s checkpoint callback.
2. Define `monitor='loss_val_nats'`, `mode='min'` so that *only one* checkpoint (the best) is written.
3. Add `max_epochs=50` so you never exceed a single ckpt/run.
4. Nightly cron job compresses these 216 best ckpts to a tarball (~10 GB) then moves to slower storage.

**checkpoint_size_mb**

- **Hook** – After each Lightning checkpoint save:

  ```python
  mb = os.path.getsize(trainer.checkpoint_callback.best_model_path) / 1e6
  self.log("checkpoint_size_mb", mb, on_step=False, on_epoch=True)
  ```

- **Validation** – First run should report ~44 MB for ResNet‑18; BN‑off shrinks slightly.

## 9. Integration into Hydra & configs

Add these callback switches to `configs/callbacks.yaml`:

```yaml
callbacks:
  - loss_fit  # knee + slope, already planned
  - curvature if=log_curvature  # lambda_max & trace
  - noise_metrics if=log_noise  # PSD & grad_var
```

Then enable via CLI:
 `python train.py log_curvature=true log_noise=true log_every_n_steps=1`

## 10. Pre‑flight validation recipe (local)

1. Set `fast_dev_run=false max_epochs=3` on a single GPU.
2. Confirm CSV rows have all keys after epoch 1.
3. Time curvature callback; it should add < 3 s per invocation.
4. Open `analysis/quick_check.ipynb`, run cells:
   - prints mean(`lambda_max`) > mean(`psd_tail`) (basic sanity)
   - asserts `lr` starts at warm‑up value then rises.
   - plots first 500 steps loss curve—visually linear in log‑log after knee.
5. If all checks pass, commit and push; CI tests should still pass. 

------

## 11. What if something looks wrong?

- **λ₁ constant ≈ 0** → check you’re sending the right mini‑batch to curvature callback (use the same pre‑processing as main loop).
- **psd_tail NaN** → buffer length < `fft_size`; increase warm‑up FFT guard.
- **Logging stalls** → ensure your logger backend (e.g., wandb) is in **async mode** or increase `env WANDB__BUFFER=1000`.

Log once, analyse forever.  With these hooks wired and validated you can replay *any* new analysis script—on future Track A–C sweeps or historical runs—without modifying the trainer again.

# Offline‑Analysis Blueprint

(*write once, reuse forever – integrates with **deconCNN/dr_exp** codebase* *)

Below you will find four sections:

1. **Library‑function stubs** you should implement (`analysis_lib/…`).
2. **Verification analyses** to sanity‑check each function on one or two pilot runs.
3. **Slide‑grade analyses** that generate every figure in the H1/H2 deck.
4. A short **self‑interrogation checklist** to ensure you understand the why‑and‑how before presenting.

Keep all plots in pure Matplotlib (no seaborn styles) so journal colours match later papers.

------

## 1 · Core analysis functions to implement

Write these in `analysis_lib/`.  Each should accept a **pandas DataFrame** that came from a single run’s log CSV; return Python scalars or DataFrames so notebooks can compose higher‑level plots.

- `knee_epoch(df, loss_col="loss_train_nats") -> int`
   *Uses* `numpy.gradient`, `scipy.stats.linregress`, AIC utility. Returns the first epoch where single‑power AIC beats exponential by ≥ 2.
- `fit_exponential(df, col, start_idx, end_idx) -> (a,b,R2)`
   Fit `log(L)=a+b·t` in log‑linear space using **`statsmodels.api.OLS`**; return slope `b`, intercept `a`, and R².
- `fit_power(df, col, start_idx, end_idx) -> (a,alpha,R2)`
   Linear fit in log‑log space.
- `fit_two_power(df, col, breaks=[.4]) -> dict`
   Segment at relative position(s) in [0,1]; run two `fit_power` calls; return dict with slopes, break index, total AIC.
- `compute_AIC(nll, k, n) -> float`
   Classic AIC = 2k + 2·nll.
- `alpha_window(df, col, epoch_start, epoch_end) -> (alpha,R2)`
   Convenience wrapper on `fit_power`.
- `ewma(series, alpha=0.3) -> np.ndarray`
   Simple exponential‑weighted moving average for λ₁ smoothing.
- `lambda_plateau_epoch(df, thr=0.05) -> int`
   Returns first epoch where λ₁’s EWMA derivative < thr for ≥ 3 epochs.
- `psd_tail(residuals, k=16) -> float`
   FFT via `torch.fft.rfft`; mean power of highest‑k bins.
- `grad_variance(running_mean, running_sqmean) -> float`
- `plot_loss_mosaic(list_of_dfs, axs)`
   Renders six subplots in log‑log: raw loss with dotted single‑power fit.

All functions should raise `ValueError` if input window length < 10 or has NaNs.

*Gotcha* – Keep all torch→numpy conversions inside a `with torch.no_grad():` block to stop the autograd engine from holding memory.

------

## 2 · Verification analyses (run before full sweep)

*Notebook `verify_callbacks.ipynb`*

1. **Knee detector sanity**
    Load one SGD run, call `knee_epoch()`. Overlay vertical knee line on loss curve. Expect knee ∈ [3,8] epochs. If < 1 or > 20, eye‑ball warm‑up LR.
2. **Fit‑family comparison**
    For the same run, compute AIC for exp, power, two‑power on [knee:50]. Expect `AIC_power < AIC_exp - 2`. Two‑power should add ≥ 2 parameters; if it still wins, flag potential two‑segment schedule effect.
3. **α window robustness**
    Loop `epoch_start ∈ {3,5,7}`, compute α; standard deviation should be < 0.02. Large swings indicate residual exponential burn‑in.
4. **λ₁ smoothing**
    Plot raw λ₁ and EWMA; EWMA should remove high‑freq jitter but preserve trend shape.
5. **PSD tail sanity**
    On one run, record `psd_tail` at epoch 3 vs epoch 30; the latter should be lower (less noise after LR decay). If higher, buffer ordering may be wrong.

------

## 3 · Publication‑grade analyses & figure specs

All notebooks live in `notebooks/` and import from `analysis_lib`.

------

### • notebook `h1_primary.ipynb`  → Primary Slide 1

- **Data** – Concatenate all runs into a single DataFrame using `glob("*/logs.csv")`.
- **Steps**
  1. For each run compute `knee_epoch`, then `fit_power` on [knee:50] → R², α_full.
  2. Build histogram `sns.histplot(R2, bins=30)` (but use plain Matplotlib).
  3. Sample six runs: three highest R², three lowest, feed to `plot_loss_mosaic`.
- **Axes**
   *Histogram* – x: “R² (power‑law fit)”, y: “# runs”. Title: “Most runs show near‑perfect linearity”.
   *Mosaic* – each sub‑axis titled with run id & R².
- **Interpretation** – > 80 % mass above 0.98 confirms H1.

------

### • notebook `h2_primary.ipynb`  → Primary Slide 2

- **Data** – Same concat.
- **Steps**
  1. For each run compute `alpha_window(5,15)` → α_early, then grab best validation CE in csv.
  2. Scatter plot α_early (x) vs best‑val‑CE (y). Over‑plot α_full points in gray. Fit Pearson r & Spearman ρ using `scipy.stats`.
- **Axes**
   x‑label: “Early slope α (epochs 5‑15)”. y‑label: “Best validation CE (nats)”. Title: “Early slopes predict final performance”. Legend handles colour by optimiser.
- **Interpretation** – Display r²; if r² ≈ 0.6‑0.7 -> strong predictor.

------

### • notebook `backup_b_fam.ipynb`  → Backup B

*Select 20 runs spanning LR×WD grid*.

- **Data** – same. Call `fit_exponential`, `fit_power`, `fit_two_power`.
- **Plot** – bar chart ΔAIC (exp vs power) per run; negative values mean power wins. Title accordingly.
- **Interpretation** – All bars below −2 → decisive evidence favouring power‑law.

------

### • notebook `backup_c_window.ipynb`

Compute Spearman ρ for α window widths; heat‑map via `matplotlib.imshow`. Colour‑bar label “|ρ(α, CE)|”.

------

### • notebook `backup_h_arch.ipynb`

Group DataFrame by `arch_combo`. For each group compute % linear runs (R²≥0.98) and α‑CE ρ; bar chart with twin y‑axes.

------

### • notebook `curvature_explore.ipynb`  *(seeds Track A/B)*

Merge λ₁ logs. Plot λ₁ plateau epoch vs knee epoch. Regression line; annotate Pearson r. Use this to decide if Curvature Plateau study is promising.

*Gotcha* – λ₁ is logged only every 500 steps; resample or interpolate to epoch grid before plateau detection.

------

## 4 · Questions to ask yourself before the meeting

1. **Conceptual clarity** – Can I explain in one breath why a power‑law is expected after stochastic mini‑batch gradients?
2. **Methodological rigour** – What assumptions underlie OLS in log‑log space, and how does Huber protect me if they break?
3. **Design justification** – For every hyper‑grid choice (LR, WD, BN), what is the *mechanistic* reason it might influence linearity?
4. **Result robustness** – If the advisor asks “what if you shift the window to epochs 10‑20?”, can I show the answer instantly?
5. **Negative cases** – Which runs *fail* H1, and do they cluster around any specific hyper‑parameter?
6. **Future path** – How do λ₁ and PSD‑tail logging today enable Tracks A‑C tomorrow?
7. **Related work anchoring** – Which prior paper first tied early slope to accuracy, and how does my dataset/architecture differ?
8. **Statistical validity** – Why do I use Spearman vs Pearson in each plot, and what does p‑value mean in this context?
9. **Narrative flow** – Can I transition from H1 to H2 in under 30 seconds without sounding scripted?
10. **Fallback plan** – If α‑CE correlation is weak, what slide do I pivot to (λ₁ plateau, two‑power fit) to keep the meeting productive?

Answer “yes” to all, and you’re ready.